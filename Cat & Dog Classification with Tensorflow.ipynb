{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat & Dog Classification with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf \n",
    "import os \n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOTUNE to set the parameters dynamically based on available CPU\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to train & test data\n",
    "train_dir = './data/train/'\n",
    "test_dir = './data/test/'\n",
    "\n",
    "# All image paths \n",
    "train_data_path = [train_dir + file_name for file_name in os.listdir(train_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Preprocess an image\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image /= 255.0\n",
    "\n",
    "    return image\n",
    "\n",
    "# Read the image from path and preprocess\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from image paths\n",
    "path_dataset = tf.data.Dataset.from_tensor_slices(train_data_path)\n",
    "\n",
    "# Create image dataset from path dataset\n",
    "image_dataset = path_dataset.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image labels\n",
    "image_label = list(map(lambda x: 1 if x.split('.')[0] == 'cat' else 0, os.listdir(train_dir)))\n",
    "\n",
    "# Create image label dataset\n",
    "label_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(image_label, tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image dataset and image label dataset\n",
    "dataset = tf.data.Dataset.zip((image_dataset, label_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, test and validation set\n",
    "DATASET_SIZE = 25000\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 4096\n",
    "\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size = SHUFFLE_BUFFER_SIZE)\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mini-batch in train_dataset and test_dataset\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def define_model():\n",
    "    # load model\n",
    "    model = tf.keras.applications.Xception(include_top=False, input_shape=(224, 224, 3))\n",
    "    # mark loaded layers as not trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # add new classifier layers\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "    output = Dense(1, activation='sigmoid')(class1)\n",
    "    # define new model\n",
    "    model = tf.keras.Model(inputs=model.inputs, outputs=output)\n",
    "    # compile model\n",
    "    opt = tf.keras.optimizers.RMSprop(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_history = model.fit(train_dataset, epochs = 50, validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('catdog_classifier_Xception.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
